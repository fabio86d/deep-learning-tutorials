{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_tutorial_cnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabio86d/deep-learning-tutorials/blob/master/tf_tutorial_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "_GaowiokAweP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TensorFlow Tutorial - CNNs\n",
        "\n",
        "In this tutorial, you will learn how to build a convolutional neural network (CNN) for image classification. We will be working with the well-known MNIST dataset featuring hand-written single digits. The CNNs task is then to identify which digit is shown on a given image.\n",
        "\n",
        "The tutorial is based on the official [TensorFlow example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py). It is meant to teach you the basic tools needed to implement a convolutional architecture in TensorFlow and it additionally shows how to leverage TensorBoard for some visualizations.\n",
        "\n",
        "In the following, we assume familiarity with the TensorFlow tutorial presented in the previous exercise, i.e., you should be aware of TensorFlow's core concepts, such as graph, session, input placeholders, etc.\n",
        "\n",
        "This tutorial consists of:\n",
        "  1. Introduction to CNNs\n",
        "  2. The MNIST Data Set\n",
        "  3. Building the Model\n",
        "  4. TensorBoard\n",
        "  5. Concluding Remarks and Exercises\n"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "XWG7kqNJAweQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction to CNNs\n",
        "### The Convolution Operation\n",
        "A convolution is a mathematical operation and represents an essential building block in image processing tasks. Depending on what filter you use to convolve an image with, you can use convolutions to blur an image (i.e., removing high-frequency noise) ...\n",
        "\n",
        "<center><img src=\"https://imgur.com/44GJQA8.png\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>\n",
        "\n",
        "... detect edges ...\n",
        "\n",
        "<center><img src=\"https://imgur.com/KIfP80j.png\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>\n",
        "\n",
        "... and [many other things](http://setosa.io/ev/image-kernels/). You can think of a convolution as sliding a window, also called _kernel_ or _filter_, over each pixel of an image and computing a dot product between the filter's values and the image's values that the filter is covering. This operation produces one output value for every location on the image over which we slide the filter. Usually, we go through every pixel in the image and position the filter such that its center pixel lies on the image pixel. Hence, for pixels lying on the boundary of the image, we have to pad the image as the filter otherwise \"spills over\" (more on this later). A visualization of the convolution process looks like this (this and subsequent animations taken from [here](https://github.com/vdumoulin/conv_arithmetic)):\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/pTNYQE7.gif\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>\n",
        "\n",
        "Here, blue is the input image, grey is the 3-by-3 filter that we are convolving the input image with and green is the output image. The dashed pixels represent padded regions."
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "2vg5xpWIAweQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Convolutions in Neural Networks\n",
        "In the context of CNNs, we are using exactly the same convolution operation, but we think of it in a slightly different way: Instead of producing a desired output image like in traditional image processing tasks (e.g. blurry, edges highlighted, etc.), a filter extracts certain _features_ from a local neighborhood and we store them in _feature maps_, sometimes also called _activation maps_ or simply _channels_. Moreover, we are moving several filters over a given image, so each convolutional layer potentially outputs several of those feature maps. Importantly, the weights of the filter are not fixed - those values are actually what the network must optimize. In other words, the network learns to set up those filters in such a way that they extract features from the images that are most useful for the network to solve the task at hand.\n",
        "\n",
        "The following animation taken from the lecture slides summarizes all this:\n",
        "\n",
        "<img src=\"https://imgur.com/JpUiKVQ.gif\" align=\"middle\" hspace=\"20px\" vspace=\"5px\">"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "164k8q9DAweS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Types of Convolutions\n",
        "When adding a convolutional layer, we must decide upon the following:\n",
        "  - **Filter Size**: We have to decide how big a filter is, i.e. determine its width and height. Common choices are  small, square, and odd, e.g. 3-by-3, 5-by-5, 7-by-7, etc. Of course, this depends on the problem you are trying to solve. Increasing the filter sizes increases the amount of trainable parameters. CNNs are every effective models thanks to **weight-sharing** and **repetition of convolution operation**, which also provides invariance to certain image operations such as translation and rotation. The parameter space can be reduced considerably in comparison to fully connected layers. Choosing huge filter sizes goes against this intuition, and in recent research it was shown, that instead of increasing filter sizes, creating deeper models is generally a better idea.\n",
        "  - **Number of Feature Maps**: We should also decide how many feature maps each layer outputs. Again, this design choice depends on the problem.\n",
        "  - **Padding**: As mentioned above, when we apply the filter on the boundary of the image, the filter \"spills over\". Hence, we must decide what to do in these cases. TensorFlow knows two options: _VALID_ or _SAME_. When we choose _VALID_ the filter will only be placed on pixels where it does not \"spill over\" the boundary. This means, that the output image will _not_ have the same size as the input image. On the other hand _SAME_ applies just enough padding that the output image will be the same size as the input (if the stride is 1).\n",
        "  - **Strides**: So far we always assumed that once we computed the output of a filter at a given location, we just move on the pixel right next to it. We could however also choose to omit some pixels inbetween. E.g., if we were to compute the convolution only on every other pixel, we would say that we use a stride of 2. This effectively reduces the size of the output image. Sometimes strided convolutions are used instead of pooling layers. The following example shows a convolution with the stride set to 2 on both the vertical and the horizontal axis of the image.\n",
        "  <center><img src=\"https://imgur.com/kjgc33A.gif\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>\n",
        "  \n",
        "  - **Dilations**: In dilated convolutions, sometimes also called \"Ã  trous\", we introduce holes in the filter, i.e. we spread the filter over a wider area but without considering some pixels inside that area in the computation of the dot product. This allows for a **faster growth of the receptive field** in deeper layers than with standard convolutions. The intuition behind is that it is easier to integrate global context into the convolution operation. The following example shows a dilated convolution with a dilation factor of 2 on both the vertical and horizontal axis of the image.\n",
        "<center><img src=\"https://imgur.com/CVVof0p.gif\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Uz3GAJTCAweT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Building Blocks of a CNN\n",
        "CNNs built for classification tasks typically make use the following types of layers:\n",
        "  - **Convolutional Layers**: Layers implementing the actual convolution as explained above. Their outputs are feature maps which are then passed through an activation function in order to introduce non-linearities into the system. Convolutional layers can be seen as extracting features that are passed on deeper into the model thus enabling the model to learn higher-level features that maket he classification task easier.\n",
        "  - **Pooling Layers**: Downsampling or pooling layers concentrate the information so that deeper layers focus more on abstract/high-level patterns. You can apply strided convolutions to apply downsampling. A decreased image size also speeds up the processing time in general because less convolutions are necessary on subsequent layers. Furthermore, pooling allows for some translation invariance on the input. A common choice is max-pooling, where only the maximum value occurring in a certain region is propagated to the output.\n",
        "  - **Dense Layers**: A dense or fully-connected layer connects every node in the input to every node in the output. This is the type of layer you already used for the linear regression model in the previous tutorial. If the input dimension is large, the amount of learnable parameters introduced by using a dense layer can quickly explode. Hence, dense layers are usually added on deeper levels of the model, where the pooling operations have already reduced the dimensionality of the data. Typically, the dense layers are added last in a classification model, performing the actual classification on the features extracted by the convolutional layers.\n",
        "  \n",
        "As an example, here is the architecture overview of the VGG16 model ([source](https://www.safaribooksonline.com/library/view/machine-learning-with/9781786462961/21266fa5-9e3b-4f9e-b3c6-2ca27a8f8c12.xhtml)).\n",
        "\n",
        "<center><img src=\"https://imgur.com/sc9tqx6.png\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>"
      ]
    },
    {
      "metadata": {
        "id": "ZfRJ0gtFOo0A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "sdM5VuhuAweU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The MNIST Data Set\n",
        "With this brief recap of convolutional architectures, we are now ready to tackle the problem of hand-written digit classification from images. Let's first have a look at the contents of the MNIST data set. To do so, let's import all the libraries we need for this tutorial and define some useful helper functions."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "1UMn2q56AweV",
        "colab_type": "code",
        "outputId": "7650dd64-4a86-43d0-c2fb-bbd577bceaa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import os\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# Tensorboard in Colab environment.\n",
        "!pip install tensorboardcolab\n",
        "from tensorboardcolab import *\n",
        "tbc = TensorBoardColab()\n",
        "\n",
        "# Visit the URL below after Tensorboard section."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n",
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://c4655994.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "v_rUq8L8AweX",
        "colab_type": "code",
        "outputId": "e6a4fe8c-6080-4e3d-eb01-7653fd68fc32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "cell_type": "code",
      "source": [
        "def plot_images(images, cls_true, cls_pred=None):\n",
        "    \"\"\"Plot 9 MNIST sample images in a 3x3 sub-plot.\"\"\"\n",
        "    assert len(images) == len(cls_true) == 9\n",
        "    \n",
        "    # Create figure with 3x3 sub-plots.\n",
        "    fig, axes = plt.subplots(3, 3)\n",
        "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "\n",
        "    # Plot image.\n",
        "    ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
        "\n",
        "    # Show true and predicted classes.\n",
        "    if cls_pred is None:\n",
        "        xlabel = \"True: {0}\".format(cls_true[i])\n",
        "    else:\n",
        "        xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
        "\n",
        "    ax.set_xlabel(xlabel)\n",
        "\n",
        "    # Remove ticks from the plot.\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-5bb0da0a741a>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    ax.imshow(images[i].reshape(img_shape), cmap='binary')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "vFxgGnxNAwea",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import the data\n",
        "log_dir = \"/tmp/tensorflow/mnist_cnn/logs\"\n",
        "mnist = input_data.read_data_sets(log_dir, one_hot=False, fake_data=False)\n",
        "\n",
        "# We know that MNIST images are 28 pixels in each dimension.\n",
        "img_size = 28\n",
        "# Images are stored in one-dimensional arrays of this length.\n",
        "img_size_flat = img_size * img_size\n",
        "# Tuple with height and width of images used to reshape arrays.\n",
        "img_shape = (img_size, img_size)\n",
        "# Images are gray-scale, so we only have one image channel\n",
        "num_image_channels = 1\n",
        "# Number of classes, one class for each of 10 digits.\n",
        "num_classes = 10\n",
        "\n",
        "# Print some stats\n",
        "print(\"Size of:\")\n",
        "print(\"- Training-set:\\t\\t{}\".format(len(mnist.train.labels)))\n",
        "print(\"- Test-set:\\t\\t{}\".format(len(mnist.test.labels)))\n",
        "print(\"- Validation-set:\\t{}\".format(len(mnist.validation.labels)))\n",
        "\n",
        "# Get some sample images from the test set.\n",
        "images = mnist.test.images[0:9]\n",
        "\n",
        "# Get the true classes for those images.\n",
        "cls_true = mnist.test.labels[0:9]\n",
        "\n",
        "# Plot the images and labels using our helper-function above.\n",
        "plot_images(images=images, cls_true=cls_true)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "-GfGgUJiAwef",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building the Model\n",
        "Let's now have a look at the core of this tutorial, namely how to build the actual CNN that is trained to predict the hand-written number on 28-by-28 gray-scale images. To begin with, when you build a model there are usually some design choices and hyper-parameters that you want to experiment with. Hence, it is good practice to make those parameters configurable through the command line or an external configuration file. TensorFlow provides  built-in support for this, called `FLAGS`, so let's define some of those: "
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "colab_type": "code",
        "id": "HkydmLp0Znso",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "tf.app.flags.DEFINE_string(\"log_dir\", log_dir, \"Summaries log directory\")\n",
        "tf.app.flags.DEFINE_string(\"feature_map_sizes\", \"32,64,128\", \"Number of layers to be used and number of feature maps per layer\")\n",
        "tf.app.flags.DEFINE_string(\"filter_sizes\", \"5,5,5\", \"Size of square filters per layer\")\n",
        "tf.app.flags.DEFINE_float(\"learning_rate\", 1e-3, \"Learning rate (default: 1e-3)\")\n",
        "tf.app.flags.DEFINE_integer(\"batch_size\", 128, \"Batch size (default: 128)\")\n",
        "tf.app.flags.DEFINE_integer(\"max_steps\", 10000, \"Number training steps/iterations (default: 1000)\")\n",
        "tf.app.flags.DEFINE_integer(\"evaluate_every_step\", 250, \"Evaluate model on validation set after this many steps/iterations (i.e., batches) (default: 50)\")\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')  # Dummy entry so that colab doesn't complain."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "colab_type": "code",
        "id": "FdEE3KXRZm_g",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "FLAGS = tf.app.flags.FLAGS\n",
        "print(\"\\nCommand-line Arguments:\")\n",
        "for key in FLAGS.flag_values_dict():\n",
        "  print(\"{:<22}: {}\".format(key.upper(), FLAGS[key].value))\n",
        "print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "_A1q5tVmAwes",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We define some functions that allow us to create `tf.Variables`. Remember that TensorFlow variables are just special tensors, that retain their value across different runs of the graph. Additionally, they are trainable, i.e., the optimizer will change their values during backpropagation, so in the end they represent all the trainable parameters of the model that we want to optimize for."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "c-3J8ifWAwet",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def weight_variable(shape):\n",
        "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
        "    # Initialize variable by drawing from a Gaussian distribution.\n",
        "    # Here, another popular choice is the use Xavier initializers.\n",
        "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
        "\n",
        "def bias_variable(shape):\n",
        "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
        "    return tf.Variable(tf.constant(0.1, shape=shape))\n",
        "\n",
        "def variable_summaries(var):\n",
        "    \"\"\"Attach a lot of summaries to a Tensor for TensorBoard visualizations.\"\"\"\n",
        "    mean = tf.reduce_mean(var)\n",
        "    tf.summary.scalar('mean', mean)\n",
        "    with tf.name_scope('stddev'):\n",
        "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "    tf.summary.scalar('stddev', stddev)\n",
        "    tf.summary.scalar('max', tf.reduce_max(var))\n",
        "    tf.summary.scalar('min', tf.reduce_min(var))\n",
        "    tf.summary.histogram('histogram', var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Ims9BAPuAwev",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using these functions, we can now define the core of our model. Note that the following functions just create nodes in the computational graph, no actual computation is taking place just yet."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "NoHydyQ2Awew",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dense_layer(input_tensor, output_dim, layer_name, act=tf.nn.relu):\n",
        "    \"\"\"\n",
        "    Reusable code for making a simple dense layer connected to the last dimension of `input_tensor`.\n",
        "    It does a matrix multiply, bias add, and then uses an activation to nonlinearize. It also sets\n",
        "    up name scoping so that the resultant graph is easy to read, and adds a number of summary ops.\n",
        "    \n",
        "    :param input_tensor: The input tensor to this layer.\n",
        "    :param output_dim: The desired output size we want to map to.\n",
        "    :param layer_name: A name for this layer.\n",
        "    :param act: Activation function used on the output of the dense layer.\n",
        "    :return: The activated output of this layer and its weights.\n",
        "    \"\"\"\n",
        "    # Adding a name scope ensures logical grouping of the layers in the graph.\n",
        "    with tf.name_scope(layer_name):\n",
        "        # Get the input dimensionality\n",
        "        input_dim = input_tensor.get_shape()[-1].value\n",
        "        \n",
        "        # Note that calling `tensor.get_shape()` retrieves the so called static shape of the tensor.\n",
        "        # The static shape is known at compile time. Some tensor dimensions can be variable, i.e.\n",
        "        # they are only defined during runtime. To retrieve the dynamic shape, i.e. the shape of\n",
        "        # a tensor when actually running computations in the graph, we should call `tf.shape(tensor)`\n",
        "        # instead.\n",
        "        \n",
        "        # This Variable will hold the state of the weights for the layer\n",
        "        with tf.name_scope('weights'):\n",
        "            weights = weight_variable([input_dim, output_dim])\n",
        "            variable_summaries(weights)\n",
        "        with tf.name_scope('biases'):\n",
        "            biases = bias_variable([output_dim])\n",
        "            variable_summaries(biases)\n",
        "        with tf.name_scope('Wx_plus_b'):\n",
        "            preactivate = tf.matmul(input_tensor, weights) + biases\n",
        "            tf.summary.histogram('pre_activations', preactivate)\n",
        "        activations = act(preactivate, name='activation')\n",
        "        tf.summary.histogram('activations', activations)\n",
        "        return activations, weights\n",
        "    \n",
        "\n",
        "def conv_layer(input_layer, filter_size, num_filters, layer_name, use_pooling=True):\n",
        "    \"\"\"\n",
        "    Adds a convolutional layer to `input_layer`. Produces an output tensor of shape\n",
        "    `[batch_size, input_height/k, input_width/k, num_filters]` where `k = 2` if\n",
        "    `use_pooling` is activated or 1 otherwise.\n",
        "    \n",
        "    :param input_layer: The input to this layer. Expected is a tensor of shape\n",
        "      `[batch_size, input_height, input_width, input_channels]`\n",
        "    :param filter_size: Width and height of the filter (scalar).\n",
        "    :param num_filters: How many feature maps to produce with this layer.\n",
        "    :param layer_name: A name for this layer.\n",
        "    :param use_pooling: Use 2x2 max-pooling if True.\n",
        "    :return: The output of this layer and the filter weights.\n",
        "    \"\"\"    \n",
        "    with tf.name_scope(layer_name):\n",
        "        # First determine the input channel size\n",
        "        num_input_channels = input_layer.get_shape()[-1].value\n",
        "        \n",
        "        # Shape of the filter-weights for the convolution.\n",
        "        # This format is determined by the TensorFlow API.\n",
        "        shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
        "\n",
        "        # Create new weights aka. filters with the given shape.\n",
        "        weights = weight_variable(shape=shape)\n",
        "\n",
        "        # Create new biases, one for each filter.\n",
        "        biases = bias_variable(shape=[num_filters])\n",
        "\n",
        "        # Create the TensorFlow operation for convolution.\n",
        "        # Note the strides are set to 1 in all dimensions.\n",
        "        # The first and last stride must always be 1,\n",
        "        # because the first is for the image-number and\n",
        "        # the last is for the input-channel.\n",
        "        # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
        "        # is moved 2 pixels across the x- and y-axis of the image.\n",
        "        # The padding is set to 'SAME' which means the input image\n",
        "        # is padded with zeroes so the size of the output is the same.\n",
        "        layer = tf.nn.conv2d(input=input_layer,\n",
        "                             filter=weights,\n",
        "                             strides=[1, 1, 1, 1],\n",
        "                             padding='SAME')\n",
        "\n",
        "        # Add the biases to the results of the convolution.\n",
        "        # A bias-value is added to each filter-channel.\n",
        "        layer += biases\n",
        "\n",
        "        # Use pooling to down-sample the image resolution?\n",
        "        if use_pooling:\n",
        "            # This is 2x2 max-pooling, which means that we\n",
        "            # consider 2x2 windows and select the largest value\n",
        "            # in each window. Then we move 2 pixels to the next window.\n",
        "            layer = tf.nn.max_pool(value=layer,\n",
        "                                   ksize=[1, 2, 2, 1],\n",
        "                                   strides=[1, 2, 2, 1],\n",
        "                                   padding='SAME')\n",
        "\n",
        "        # Rectified Linear Unit (ReLU).\n",
        "        # It calculates max(x, 0) for each input pixel x.\n",
        "        # This adds some non-linearity to the formula and allows us\n",
        "        # to learn more complicated functions.\n",
        "        layer = tf.nn.relu(layer)\n",
        "\n",
        "        # Note that ReLU is normally executed before the pooling,\n",
        "        # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
        "        # save 75% of the relu-operations by max-pooling first.\n",
        "\n",
        "        # We return both the resulting layer and the filter-weights\n",
        "        # because we will plot the weights later.\n",
        "        return layer, weights\n",
        "\n",
        "    \n",
        "def flatten_layer(layer):\n",
        "    \"\"\"\n",
        "    A helper function to flatten the output of a convolutional layer. As a conv layer\n",
        "    outputs a 4-dimensional tensor, we need to reduce it to 2 dimensions so that\n",
        "    we can use it as an input the a dense layer.\n",
        "    \n",
        "    :param layer: The output of a convolutional layer.\n",
        "    :return: The input layer flattened to have shape `[batch_size, num_features]`\n",
        "    \"\"\"\n",
        "    # Get the shape of the input layer.\n",
        "    layer_shape = layer.get_shape()\n",
        "\n",
        "    # The shape of the input layer is assumed to be:\n",
        "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
        "\n",
        "    # The number of features is: img_height * img_width * num_channels\n",
        "    # We can use a function from TensorFlow to calculate this.\n",
        "    num_features = layer_shape[1:4].num_elements()\n",
        "    \n",
        "    # Reshape the layer to [batch_size, num_features].\n",
        "    # Note that we just set the size of the second dimension\n",
        "    # to num_features and the size of the first dimension to -1\n",
        "    # which means the size in that dimension is calculated\n",
        "    # so the total size of the tensor is unchanged from the reshaping.\n",
        "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
        "\n",
        "    # The shape of the flattened layer is now:\n",
        "    # [num_images, img_height * img_width * num_channels]\n",
        "\n",
        "    # Return both the flattened layer and the number of features.\n",
        "    return layer_flat, num_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ke7bADlYAwez",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With this we can now construct the model. The last thing we need to do so, is set up placeholders through which we can feed values into the model. Recall that placeholders are just special tensors for which TensorFlow checks that they have been supplied appropriately when ever running a computation in the graph. Note that the first dimension of each placeholder is `None`. This means that the actual size of that dimension is unknown at compile time and thus can vary at runtime. This is a useful feature especially for the batch size."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "KF1baNcEAwe0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create input placeholders\n",
        "with tf.name_scope('input'):\n",
        "    # This is the format how we read MNIST images.\n",
        "    images_flat = tf.placeholder(tf.float32, [None, img_size_flat], name='x-input')\n",
        "    # We reshape this so that we can feed it directly into a conv layer.\n",
        "    data_placeholder = tf.reshape(images_flat, [-1, img_size, img_size, num_image_channels])\n",
        "    # Also create a placeholder for the target labels\n",
        "    label_placeholder = tf.placeholder(tf.int32, [None], name='y-input')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Lprw5nSZAwe3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract design parameters from the command line\n",
        "feature_map_sizes = list(map(int, FLAGS.feature_map_sizes.split(',')))\n",
        "filter_sizes = list(map(int, FLAGS.filter_sizes.split(',')))\n",
        "assert len(filter_sizes) == len(feature_map_sizes)\n",
        "\n",
        "# Build the actual model\n",
        "next_in = data_placeholder\n",
        "weights = []\n",
        "layer_outs = []\n",
        "for i, (num_out_channels, filter_size) in enumerate(zip(feature_map_sizes, filter_sizes)):\n",
        "    next_in, w = conv_layer(next_in, filter_size, num_out_channels, 'conv{}_layer'.format(i), use_pooling=True)\n",
        "    weights.append(w)\n",
        "    layer_outs.append(next_in)\n",
        "    \n",
        "# Flatten the output\n",
        "out_flat, _ = flatten_layer(next_in)\n",
        "# Add a dense layer with 10 output neurons, i.e. one for every class from 0-9\n",
        "logits, _ = dense_layer(out_flat, num_classes, 'dense_layer', act=tf.identity)\n",
        "probs = tf.nn.softmax(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ebPP83eSAwe5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that we did not use an activation function on the output of the dense layer. Generally speaking, it is not always reasonable to use an activation function on the last layer, depending on the distribution of your outputs. For example, if you used a `tf.nn.tanh` activation on your output layer, all outputs would come to lie between -1 and 1. While an activation function on the outputs is a trick to define the output range for free, it is inherently biased. In other words, the distribution of output values does not match with the shape of the activation function. Depending on your problem, this might or might not make sense. \n",
        "\n",
        "For our case, we want the outputs of the model to be probabilities, i.e., the model should tell us what is the probability that a certain image belongs to each of the 10 classes. For this, we can use the softmax activation function, defined as follows:\n",
        "\n",
        "$$\n",
        "\\sigma(\\mathbf{z})_j = \\frac{e^{\\mathbf{z}_j}}{\\sum_{k=1}^K e^{\\mathbf{z}_k}}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{z}$ is our $K$-dimensional output vector `logits` and $K$ refers to the number of classes that we are trying to predict, i.e. $K=10$ in our case. The softmax function essentially squashes its input between 0 and 1 and makes sure that all $K$ values sum up to 1. In other words, it produces a valid probability distribution over the number of classes.\n",
        "\n",
        "<center><img src=\"https://imgur.com/te7R6BW.png\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>\n",
        "\n",
        "So, how come we did not supply `tf.nn.softmax` to our dense layer above? The reason is that when we feed softmax-activated values to our loss function, we introduce numerical instabilities that destabilize the training. The loss function measures how good the prediction of our network is. Ideally, if we feed an image depicting a hand-written 3 to our model, we want it to assign a probability of 1 to the class 3 and probabilities of 0 to all other classes. Thus, for every image we have a target distribution $q$, which is just a one-hot encoding of its label, and an estimated probability distribution $p$ which is the output of the model. A one-hot encoding of a label is simply a vector of zeros that has exactly one entry showing 1 corresponding to the index of that label. For example, the one-hot encoding of label 3 looks like this:\n",
        "\n",
        "$$\n",
        "\\left[0, 0, 0, 1, 0, 0, 0, 0, 0, 0 \\right]^T \\in \\mathbb{R}^{10}\n",
        "$$\n",
        "\n",
        "The only thing left is now to find a measure of the distance between those two distributions, i.e. how closely $p$ resembles the one-hot encoding $q$. For this, we can use the cross-entropy:\n",
        "\n",
        "$$\n",
        "H(p, q) = H(p) + D_{KL}(p || q) = -\\sum\\limits_x p(x) \\log q(x)\n",
        "$$\n",
        "\n",
        "where $H(p)$ is the entropy of $p$ and $D_{KL}$ is the Kullback-Leibler Divergence. You can see from this formula that if the predicted probability $p$ is exactly a one-hot encoding, $H(p, q)$ will be 0, because the entropy $H(p)$ of a one-hot vector is 0 and the KL divergence will also be 0 because $p$ exactly matches $q$.\n",
        "\n",
        "The problem with the cross-entropy, as mentioned before, is that it is numerically unstable and can produce `inf` values during training. Hence, TensorFlow produces a more stable version which takes as input the logits, _not_ the softmax activated values. This is why we did not choose an activation function for the outputs of the dense layer."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "IvdV6zNeAwe6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loss: Cross-Entropy\n",
        "with tf.name_scope('cross_entropy'):\n",
        "    # So here we use tf.nn.softmax_cross_entropy_with_logits on the\n",
        "    # raw outputs of the nn_layer above, and then average across\n",
        "    # the batch.\n",
        "    cross_entropy_loss = tf.reduce_mean(\n",
        "        tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label_placeholder))\n",
        "    tf.summary.scalar('cross_entropy_loss', cross_entropy_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "iib0R9ofAwe9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we defined the model and the loss, we must also choose the optimizer. Before doing so, it might be a good idea to check how many trainable parameters we've created with the model definition above. This is both a sanity check and also gives you an intuition about the capacity of the model."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "o__9ViFFAwe9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def count_trainable_parameters():\n",
        "    \"\"\"Counts the number of trainable parameters in the current default graph.\"\"\"\n",
        "    tot_count = 0\n",
        "    for v in tf.trainable_variables():\n",
        "        v_count = 1\n",
        "        for d in v.get_shape():\n",
        "            v_count *= d.value\n",
        "        tot_count += v_count\n",
        "    return tot_count\n",
        "print(\"Number of trainable parameters: {}\".format(count_trainable_parameters()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Ru8lrnniAwfC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With this, it is time to define our optimizer."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "uIi8Sl8LAwfC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a variable to contain a counter for the global training step.\n",
        "global_step = tf.Variable(1, name='global_step', trainable=False)\n",
        "\n",
        "# Optimization operation: SGD (Stochastic Gradient Descent)\n",
        "with tf.name_scope('train'):\n",
        "    # This operation automatically increases the `global_step` by 1 every time it is called\n",
        "    train_step = tf.train.GradientDescentOptimizer(FLAGS.learning_rate).minimize(\n",
        "        cross_entropy_loss, global_step=global_step)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "laIPCcJdAwfG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Additionally want to monitor the accuracy of the training, not just the cross-entropy value.\n",
        "with tf.name_scope('accuracy'):\n",
        "    with tf.name_scope('correct_predictions'):\n",
        "        #predictions = tf.argmax(logits, 1, name=\"predictions\")\n",
        "        correct_predictions = tf.nn.in_top_k(logits, label_placeholder, 1)\n",
        "    with tf.name_scope('accuracy'):\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "    tf.summary.scalar('accuracy', accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "DlGkkZeCAwfK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At this point we are almost ready to start the training. The only thing left to do is start the session (hopefully on the GPU), add some helper functions to feed data into the model and prepare some summary writers so that we can monitor the progress in TensorBoard."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "wd6qhWoWAwfQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create the session\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "# Tensorboard\n",
        "train_writer = tbc.get_deep_writers(\"single_layer_model/train\")\n",
        "train_writer.add_graph(sess.graph)\n",
        "valid_writer = tbc.get_deep_writers(\"single_layer_model/valid\")\n",
        "valid_writer.add_graph(sess.graph)\n",
        "\n",
        "# Initialize all variables\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# To be able to see something in tensorboard, we must merge summaries to one common operation.\n",
        "# Whenever we want to write summaries, we must request this operation from the graph.\n",
        "# Note: creating the file writers should happen after the session was launched.\n",
        "summaries_merged = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "7V5WXpnsAwfM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def feed_dict(train_mode):\n",
        "    \"\"\"Make a TensorFlow feed_dict: maps data onto placeholders.\"\"\"\n",
        "    if train_mode:\n",
        "        xs, ys = mnist.train.next_batch(FLAGS.batch_size)\n",
        "    else:\n",
        "        xs, ys = mnist.validation.images, mnist.validation.labels\n",
        "    return {images_flat:xs, label_placeholder:ys}\n",
        "\n",
        "def do_train_step(num_steps, summary_op):\n",
        "    \"\"\"Perform as many training steps as specified and may be evaluate on validation set.\"\"\"\n",
        "    for i in range(num_steps):\n",
        "        step = tf.train.global_step(sess, global_step)\n",
        "        if step % FLAGS.evaluate_every_step == 0:\n",
        "            # Record summaries and test-set accuracy\n",
        "            summary, acc_valid = sess.run([summary_op, accuracy], feed_dict=feed_dict(False))\n",
        "            valid_writer.add_summary(summary, step)\n",
        "            print('[{}] Accuracy Training [{:.3f}], Validation [{:.3f}]'.format(step, acc_valid, acc_train)) \n",
        "\n",
        "        # Record train set summaries, and train\n",
        "        summary, acc_train, _ = sess.run([summary_op, accuracy, train_step], feed_dict=feed_dict(True))\n",
        "        train_writer.add_summary(summary, step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6U7AhbttxH9m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "-dTGRNSqAwfS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's train this model for a couple of steps. "
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "nl3AgHHCAwfT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "do_train_step(1001, summaries_merged)\n",
        "train_writer.flush()\n",
        "valid_writer.flush()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oTMOMTJcj0Pj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You will see training loss, distribution of weight & bias parameter values and our graph."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "GsynaqHvAwfV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can see that the accuracy on the validation set steadily increases. Sometimes it might be interesting to see some visualizations of the learned convolutional filter weights or the outputs of layer. Let's define some helper functions to do that."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "TjGPxdcQAwfW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_conv_weights(session, weights, input_channel=0):\n",
        "    \"\"\"Helper-function for plotting convolutional weights.\"\"\"\n",
        "    # Assume weights are TensorFlow ops for 4-dim variables\n",
        "    # e.g. weights_conv1 or weights_conv2.\n",
        "    \n",
        "    # Retrieve the values of the weight-variables from TensorFlow.\n",
        "    # A feed-dict is not necessary because nothing is calculated.\n",
        "    w = session.run(weights)\n",
        "\n",
        "    # Get the lowest and highest values for the weights.\n",
        "    # This is used to correct the colour intensity across\n",
        "    # the images so they can be compared with each other.\n",
        "    w_min = np.min(w)\n",
        "    w_max = np.max(w)\n",
        "\n",
        "    # Number of filters used in the conv. layer.\n",
        "    num_filters = w.shape[3]\n",
        "\n",
        "    # Number of grids to plot.\n",
        "    # Rounded-up, square-root of the number of filters.\n",
        "    num_grids = math.ceil(math.sqrt(num_filters))\n",
        "    \n",
        "    # Create figure with a grid of sub-plots.\n",
        "    fig, axes = plt.subplots(num_grids, num_grids)\n",
        "\n",
        "    # Plot all the filter-weights.\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        # Only plot the valid filter-weights.\n",
        "        if i<num_filters:\n",
        "            # Get the weights for the i'th filter of the input channel.\n",
        "            # See new_conv_layer() for details on the format\n",
        "            # of this 4-dim tensor.\n",
        "            img = w[:, :, input_channel, i]\n",
        "\n",
        "            # Plot image.\n",
        "            ax.imshow(img, vmin=w_min, vmax=w_max,\n",
        "                      interpolation='nearest', cmap='seismic')\n",
        "        \n",
        "        # Remove ticks from the plot.\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    \n",
        "    # Ensure the plot is shown correctly with multiple plots\n",
        "    # in a single Notebook cell.\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "def plot_conv_layer(layer, image):\n",
        "    \"\"\"Helper-function for plotting the output of a convolutional layer.\"\"\"\n",
        "    # Assume layer is a TensorFlow op that outputs a 4-dim tensor\n",
        "    # which is the output of a convolutional layer,\n",
        "    # e.g. layer_conv1 or layer_conv2.\n",
        "\n",
        "    # Create a feed-dict containing just one image.\n",
        "    # Note that we don't need to feed y_true because it is\n",
        "    # not used in this calculation.\n",
        "    feed_dict = {images_flat: [image]}\n",
        "\n",
        "    # Calculate and retrieve the output values of the layer\n",
        "    # when inputting that image.\n",
        "    values = sess.run(layer, feed_dict=feed_dict)\n",
        "\n",
        "    # Number of filters used in the conv. layer.\n",
        "    num_filters = values.shape[3]\n",
        "\n",
        "    # Number of grids to plot.\n",
        "    # Rounded-up, square-root of the number of filters.\n",
        "    num_grids = math.ceil(math.sqrt(num_filters))\n",
        "    \n",
        "    # Create figure with a grid of sub-plots.\n",
        "    fig, axes = plt.subplots(num_grids, num_grids)\n",
        "\n",
        "    # Plot the output images of all the filters.\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        # Only plot the images for valid filters.\n",
        "        if i<num_filters:\n",
        "            # Get the output image of using the i'th filter.\n",
        "            # See new_conv_layer() for details on the format\n",
        "            # of this 4-dim tensor.\n",
        "            img = values[0, :, :, i]\n",
        "            # Plot image.\n",
        "            ax.imshow(img, interpolation='nearest', cmap='binary')\n",
        "        \n",
        "        # Remove ticks from the plot.\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    \n",
        "    # Ensure the plot is shown correctly with multiple plots\n",
        "    # in a single Notebook cell.\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "g9VDgg4oAwfb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_conv_weights(sess, weights=weights[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "WMnFwb-6Awfd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Because we trained the model only for a few epochs, the validation accuracy is not overwhelming. Let's train more."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "2KGyXzSkAwfe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "do_train_step(10000, summaries_merged)\n",
        "plot_conv_weights(sess, weights=weights[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "4GWM06fqAwfh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once you trained the model for a longer period of time, you should achieve a validation accuracy of above 95 %. Next, we can look at how the model performs on some examples taken from the test set."
      ]
    },
    {
      "metadata": {
        "id": "u2_Ag2FdGkWE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "DFA6Z_F_Awfi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Feed some test images into the model and get the predicted label\n",
        "test_images = mnist.test.images[0:9]\n",
        "\n",
        "# Get the true classes for those images.\n",
        "test_cls_true = mnist.test.labels[0:9]\n",
        "\n",
        "# Feed the images into the model and get the predictions\n",
        "feed_dict = {images_flat: test_images}\n",
        "logits_np = sess.run(logits, feed_dict)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "R5Hs8OY2Awfk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# logits_np has shape [9, 10], find the class with the highest probability for each\n",
        "test_cls_predicted = np.argmax(logits_np, axis=-1)\n",
        "\n",
        "# then visualize\n",
        "plot_images(test_images, test_cls_true, test_cls_predicted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "E-jhIdm7Awfo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Assuming the model was trained for long enough, you should now see that the model has performed quite well on these 9 images.\n",
        "\n",
        "May be for a certain image, you are also interested in visualizing the output of a convolutional layer, i.e. its resulting feature maps. For the first image in the test data set, i.e. the one showing number 7 in the above plot, this would look like follows:"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "acZDx5MvAwfp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_conv_layer(layer_outs[0], mnist.test.images[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "nNd_S9YjAwft",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That's it - we've successfully trained a convolutional neural network for classification of hand-written digits. Lastly, let's not forget to clean up after us."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "CQQ9wXhEAwfu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# cleanup\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ph5Zy7QQAwfy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TensorBoard\n",
        "The most important one being the `accuracy` measurement. In this plot we see that the accuracy on both the validation and training set behave very similarly, which is what we want. If you would observe that the validation accuracy is again going down after some time, while the training accuracy keeps improving, this would be a strong indication that the model is overfitting.\n",
        "\n",
        "Another trend we see from this plot is that accuracy quickly improves in the beginning and then starts to saturate over time. This is usually the convergence behavior one wants to see. If you see different behavior, e.g., no improvement at all over time, or no saturation phase, you might want to think about your choices for the learning rate, batch size, type of optimizer, or - in the worst case - the model architecture entirely. This is why it is beneficial to make certain parameteres configurable through the command line, so that you can quickly try different values for different hyper-parameters.\n",
        "\n",
        "TensorBoard also interactively visualizes the computational graph - to see this, click on `GRAPHS` in the top bar. Double-click on nodes displayed in this graph to see more details. Also, there are more buttons located in the top bar of the page - feel free to explore these on your own.\n"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "X72HsH2uAwfz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Concluding Remarks and Exercises\n",
        "In this tutorial you learned how to train and evaluate a simple convolutional neural network in TensorFlow for the task of hand-written digit classification and how to visualize certain statistics of the training process by using TensorBoard. You also learned which design choices are typically required to build a CNN (size of filters, size of filter maps, strides, pooling, etc.) and how you can plot results from intermediate layers in the architecture. We used a fairly low-level API of TensorFlow to build this model. We would also like to point you to the [`tf.layers`](https://www.tensorflow.org/api_docs/python/tf/layers) or [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras) API which has some more levels of abstraction. You are free to use any Tensorflow API in your projects. If you find the low-level API overwhelming, you may have a look this [`keras tutorial`](https://www.tensorflow.org/tutorials) implementing the same task with ridicilious amount of code compared to our tutorial."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hQOlkdBUAwf1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Although the model built in this tutorial is fairly simple, you should now be able to understand more complex, state-of-the art architectures, some of which shall be listed here for your reference:\n",
        "  - [VGG](https://arxiv.org/abs/1409.1556)\n",
        "  - [DenseNet](https://arxiv.org/abs/1608.06993)\n",
        "  - [ResNet](https://arxiv.org/abs/1512.03385)\n",
        "  \n",
        "Lastly, we encourage you to play around with this notebook. As a source of inspiration, here are a few (optional) exercises that you can try to solve:\n",
        "  1. Implement strided convolutions to replace max pooling. What is the effect on the performance?\n",
        "  2. Visualize the cross-entropy loss in TensorBoard.\n",
        "  3. Play around with various architectures and compare them. For example, try to\n",
        "    1. Use larger filter sizes while keeping the amount of layers fixed.\n",
        "    2. Use more layers while keeping the filter sizes fixed.\n",
        "    3. Introduce layers whose outputs are not max-pooled.\n",
        "  \n",
        "  In these experiments, check what happens to the number of parameters, the speed of training, the convergence rate etc. and try to find a model that beats the simple CNN we trained in this tutorial."
      ]
    }
  ]
}